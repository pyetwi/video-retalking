{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1jovRJtpTNgUuOBoiMqPnwxHrEiKx7sp5?usp=drive_link](https://colab.research.google.com/drive/1jovRJtpTNgUuOBoiMqPnwxHrEiKx7sp5?usp=drive_link)\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVfkv2BXSpr3"
      },
      "source": [
        "## VideoReTalkingï¼šAudio-based Lip Synchronization for Talking Head Video Editing In the Wild (ORIGINAL SOURCE)\n",
        "\n",
        "[Arxiv](https://arxiv.org/abs/2211.14758) | [Project](https://vinthony.github.io/video-retalking/) | [Github](https://github.com/vinthony/video-retalking)\n",
        "\n",
        "Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, Nannan Wang\n",
        "\n",
        "Xidian University, Tencent AI Lab, Tsinghua University\n",
        "\n",
        "*SIGGRAPH Asia 2022 Conferenence Track*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9hdPaH6UL_F"
      },
      "source": [
        "Project Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnKT9goiQ3Hk"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "!python --version\n",
        "!apt-get update\n",
        "!apt install ffmpeg &> /dev/null\n",
        "\n",
        "!git clone https://github.com/pyetwi/video-retalking.git &> /dev/null\n",
        "%cd video-retalking\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "### For this project, we curated subsets of LibriSpeech (real audio), LibriSevoc (fake audio), FakeAVCeleb (real video)\n",
        "### We had to request permission for FakeAVCeleb (waited for approximately 1 week).\n",
        "### This involved quite a bit of shell-scripting to manually process the zip files!\n",
        "audio_fake_url = 'https://drive.google.com/uc?export=download&id=1F_8hYzlA1i9KIIrB_KzOfagIo-Ovgtfr'\n",
        "audio_real_url = 'https://drive.google.com/uc?export=download&id=1OAVkQ_Hvf7xI0y5PZILOCgJKNIISA1wW'\n",
        "video_real_url = 'https://drive.google.com/uc?export=download&id=1yyv9_z_pR3CTIwhPqNWE0aGZjbfrUDe5'\n",
        "\n",
        "# Download the files using gdown\n",
        "gdown.download(audio_fake_url, 'audio_fake.zip', quiet=False)\n",
        "gdown.download(audio_real_url, 'audio_real.zip', quiet=False)\n",
        "gdown.download(video_real_url, 'video_real.zip', quiet=False)\n",
        "\n",
        "!unzip -q -d examples/ audio_fake.zip\n",
        "!unzip -q -d examples/ audio_real.zip\n",
        "!unzip -q -d examples/ video_real.zip\n",
        "\n",
        "\n",
        "!rm -rf audio_fake.zip\n",
        "!rm -rf audio_real.zip\n",
        "!rm -rf video_real.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwJS0eaM61Cq"
      },
      "source": [
        "**Download Pretrained Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x18qYuQY678E"
      },
      "outputs": [],
      "source": [
        "!mkdir ./checkpoints  \n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/30_net_gen.pth -O ./checkpoints/30_net_gen.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/BFM.zip -O ./checkpoints/BFM.zip\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/DNet.pt -O ./checkpoints/DNet.pt\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/ENet.pth -O ./checkpoints/ENet.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/expression.mat -O ./checkpoints/expression.mat\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/face3d_pretrain_epoch_20.pth -O ./checkpoints/face3d_pretrain_epoch_20.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/GFPGANv1.3.pth -O ./checkpoints/GFPGANv1.3.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/GPEN-BFR-512.pth -O ./checkpoints/GPEN-BFR-512.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/LNet.pth -O ./checkpoints/LNet.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/ParseNet-latest.pth -O ./checkpoints/ParseNet-latest.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/RetinaFace-R50.pth -O ./checkpoints/RetinaFace-R50.pth\n",
        "!wget https://github.com/pyetwi/video-retalking/releases/download/v0.0.1/shape_predictor_68_face_landmarks.dat -O ./checkpoints/shape_predictor_68_face_landmarks.dat\n",
        "!unzip -d ./checkpoints/BFM ./checkpoints/BFM.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJRTF4U8UOjv"
      },
      "source": [
        "**Dataset Generation**\n",
        "\n",
        "For our project, Hoang Tang engineered a pipeline to generate deepfake datasets.\n",
        "\n",
        "Our dataset contains three types of videos:\n",
        "\n",
        "Type #1: These are completely legitimate videos (from subsets of FakeAVCeleb + GRID datasets), consistenting of a real video & its corresponding audio\n",
        "Type #2: These videos contain the real video, but its video has been transformed to match a randomly selected (synthetic) soundtrack from a subset of the LibriSpeech dataset.\n",
        "Type #3: These videos contain the real video, but its video has been transformed to match a randomly selected (fake) soundtrack from a subset of the LibriSevoc dataset.\n",
        "\n",
        "We manually wrote thousands of lines of shell scripts to preprocess each of the datasets, since they were all initially too big to use and were compressed. We also wanted to use the KoDF (Korean Deepfakes) dataset for our project, but it was nearly 2TB in size. Clearly, such a dataset is infeasible for usage given available resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U-IY-cBSporP"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import glob, os, sys\n",
        "import random\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "src_videos = 'examples/video_real'\n",
        "src_real_audios = 'examples/audio_real'\n",
        "src_fake_audios = 'examples/audio_fake'\n",
        "\n",
        "all_videos = [\n",
        "    os.path.basename(x) for x in glob.glob('{}/*.mp4'.format(src_videos))\n",
        "]\n",
        "\n",
        "all_real_audios = [\n",
        "    os.path.basename(x) for x in glob.glob('{}/*.wav'.format(src_real_audios))\n",
        "]\n",
        "\n",
        "all_fake_audios = [\n",
        "    os.path.basename(x) for x in glob.glob('{}/*.wav'.format(src_fake_audios))\n",
        "]\n",
        "\n",
        "### This code will generate a dataset of the following videos:\n",
        "### Type 1 videos (real): \n",
        "### - These are completely legitimate videos, consistenting of a real video & its corresponding audio\n",
        "TYPE_ONE_COUNT = 5\n",
        "type_one_dst_folder = '/examples/type_one'\n",
        "os.makedirs(type_one_dst_folder, exist_ok=True)\n",
        "\n",
        "# randomly sample TYPE_ONE_COUNT videos, with replacement.\n",
        "type_one_videos = np.random.choice(all_videos, TYPE_ONE_COUNT, replace=True)\n",
        "for video in type_one_videos:\n",
        "  print(video)\n",
        "  # !cp {type_one_src_videos}/{video} {type_one_dst_folder}/{video}\n",
        "\n",
        "\n",
        "### Type 2 videos (deepfake):\n",
        "### - These videos contain the real video, but its video has been transformed to match a randomly selected (synthetic) soundtrack from the LibriSpeech dataset.\n",
        "TYPE_TWO_COUNT = 5\n",
        "type_two_dst_folder = '/examples/type_two'\n",
        "os.makedirs(type_two_dst_folder, exist_ok=True)\n",
        "\n",
        "type_two_videos = random.sample(all_videos, TYPE_TWO_COUNT)\n",
        "type_two_audios = random.sample(all_real_audios, TYPE_TWO_COUNT)\n",
        "\n",
        "for idx, (video, audio) in enumerate(zip(type_two_videos, type_two_audios), start = 1):\n",
        "  print(video)\n",
        "  print(audio)\n",
        "\n",
        "### Type 3 videos (deepfake):\n",
        "### - These videos contain the real video, but its video has been transformed to match a randomly selected (fake) soundtrack from the LibriSevoc dataset.\n",
        "TYPE_THREE_COUNT = 5\n",
        "type_three_dst_folder = '/examples/type_three'\n",
        "os.makedirs(type_three_dst_folder, exist_ok=True)\n",
        "\n",
        "type_three_videos = random.sample(all_videos, TYPE_THREE_COUNT)\n",
        "type_three_audios = random.sample(all_fake_audios, TYPE_THREE_COUNT)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# input_video_path = 'examples/face/{}'.format(default_vid_name.value)\n",
        "# input_audio_path = 'examples/audio/{}'.format(default_audio_name.value)\n",
        "\n",
        "# !python3 inference.py \\\n",
        "#   --face {input_video_path} \\\n",
        "#   --audio {input_audio_path} \\\n",
        "#   --up_face \"surprise\" \\\n",
        "#   --outfile results/output.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MtI_R1bLJ-f"
      },
      "source": [
        "Visualize the input video and audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ljbScdofJyGO"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "input_video_name = './examples/face/{}'.format(default_vid_name.value)\n",
        "input_video_mp4 = open('{}'.format(input_video_name),'rb').read()\n",
        "input_video_data_url = \"data:video/x-m4v;base64,\" + b64encode(input_video_mp4).decode()\n",
        "print('Display input video: {}'.format(input_video_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % input_video_data_url))\n",
        "\n",
        "input_audio_name = './examples/audio/{}'.format(default_audio_name.value)\n",
        "input_audio_mp4 = open('{}'.format(input_audio_name),'rb').read()\n",
        "input_audio_data_url = \"data:audio/wav;base64,\" + b64encode(input_audio_mp4).decode()\n",
        "print('Display input audio: {}'.format(input_audio_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <audio width=400 controls>\n",
        "        <source src=\"%s\" type=\"audio/wav\">\n",
        "  </audio>\n",
        "  \"\"\" % input_audio_data_url))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7hUwRCyUYEA"
      },
      "outputs": [],
      "source": [
        "input_video_path = 'examples/face/{}'.format(default_vid_name.value)\n",
        "input_audio_path = 'examples/audio/{}'.format(default_audio_name.value)\n",
        "\n",
        "!python3 inference.py \\\n",
        "  --face {input_video_path} \\\n",
        "  --audio {input_audio_path} \\\n",
        "  --outfile results/output.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB5RbKc-njkB"
      },
      "source": [
        "Visualize the output video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ravs9UDucMfy"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# visualize code from makeittalk\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os, sys, glob, cv2, subprocess, platform\n",
        "\n",
        "def read_video(vid_name):\n",
        "  video_stream = cv2.VideoCapture(vid_name)\n",
        "  fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "  full_frames = []\n",
        "  while True:\n",
        "    still_reading, frame = video_stream.read()\n",
        "    if not still_reading:\n",
        "        video_stream.release()\n",
        "        break\n",
        "    full_frames.append(frame)\n",
        "  return full_frames, fps\n",
        "\n",
        "input_video_frames, fps = read_video(input_video_path)\n",
        "output_video_frames, _ = read_video('./results/output.mp4')\n",
        "\n",
        "frame_h, frame_w = input_video_frames[0].shape[:-1]\n",
        "out_concat = cv2.VideoWriter('./temp/temp/result_concat.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_w*2, frame_h))\n",
        "for i in range(len(output_video_frames)):\n",
        "  frame_input = input_video_frames[i % len(input_video_frames)]\n",
        "  frame_output = output_video_frames[i]\n",
        "  out_concat.write(cv2.hconcat([frame_input, frame_output]))\n",
        "out_concat.release()\n",
        "\n",
        "command = 'ffmpeg -loglevel error -y -i {} -i {} -strict -2 -q:v 1 {}'.format(input_audio_path, './temp/temp/result_concat.mp4', './results/output_concat_input.mp4')\n",
        "subprocess.call(command, shell=platform.system() != 'Windows')\n",
        "\n",
        "\n",
        "output_video_name = './results/output.mp4'\n",
        "output_video_mp4 = open('{}'.format(output_video_name),'rb').read()\n",
        "output_video_data_url = \"data:video/mp4;base64,\" + b64encode(output_video_mp4).decode()\n",
        "print('Display lip-syncing video: {}'.format(output_video_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video height=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % output_video_data_url))\n",
        "\n",
        "output_concat_video_name = './results/output_concat_input.mp4'\n",
        "output_concat_video_mp4 = open('{}'.format(output_concat_video_name),'rb').read()\n",
        "output_concat_video_data_url = \"data:video/mp4;base64,\" + b64encode(output_concat_video_mp4).decode()\n",
        "print('Display input video and lip-syncing video: {}'.format(output_concat_video_name), file=sys.stderr)\n",
        "display(HTML(\"\"\"\n",
        "  <video height=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % output_concat_video_data_url))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMPin07iIA2oewCCP9ZTz6w",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
